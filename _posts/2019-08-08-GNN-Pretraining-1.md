---
layout: article
title: GNN 教程：图上的预训练任务下篇
key: GNN_tutorial_pretraining_1
tags: GNN
category: blog
pageview: true
date: 2019-08-08 12:00:00 +08:00
---
## 引言

**此为原创文章，未经许可，禁止转载**

前一篇博文[GNN 教程：图上的预训练任务上篇](https://archwalker.github.io/blog/2019/07/18/GNN-Pretain-0.html)已经向大家介绍了一种图上预训练的方式，通过设计边重建、Centrality Score Ranking、保留图簇信息三种图预训练任务来生成节点embedding，以从局部到全局捕获节点的图结构信息；然后，将预训练模型生成的节点Embedding在特定的任务中做微调，最终应用于该特定任务中。

本博文将向大家介绍来自论文 [Pre-training Graph Neural Networks](https://arxiv.org/abs/1905.12265) 的另一种预训练方式，该论文重点讨论下面三个问题：

1. 如何生成节点 embedding，以捕获节点和它邻居的在结构的相似性(相邻节点的Embedding在投影空间中相近)；

2. 如何生成可组合(composable)的节点 embedding，这些Embedding通过pooling的方式能够刻画整个图的Embedding；

3. 如何生成领域知识(domain-knowledge)相关的节点 embedding


## 动机

虽然 GNN 模型及其变体在图结构数据的学习方面取得了成功，但是 GNNs 存在以下两个主要挑战：

1. 目前应用于特定领域的 GNNs 算法，主要属于有监督学习算法，该算法需要采用大量带标注的图数据进行训练，才能获得更高准确度的模型。然而，由于带标签数据的极度稀缺以及标注成本过高等问题，导致 GNNs 训练模型容易出现过拟合现象；
2. 在 GNNs 中，需要预测测试集中的一些图，这些图具有不同于训练集中所包含图的图结构，导致容易陷入 out-of-distribution 预测问题。举个例子，预测新合成的，与训练集分子结构不同的分子的化学属性，或者来自新物种的蛋白质的功能，其具有与先前研究的物种不同的PPI网络结构。

## 预训练策略

和[上篇论文](https://archwalker.github.io/blog/2019/07/18/GNN-Pretain-0.html)不同的是，这篇论文设计预训练任务将会涉及到node-level和graph-level两个层次上。见下左图(a)：

![](http://ww2.sinaimg.cn/large/006tNc79ly1g5skvg3ygrj31jc0gudn8.jpg)

为了学习领域知识(在论文中是分子的类别)，在节点的层次上，作者设计了masking任务，在图的层次上，作者设计了图分类的任务；为了学习图结构信息，在节点的层次上，作者设计了上下文预测的任务，在图的层次上，作者设计了图分类的任务。

(b)图解释了作者结合节点层次和图层次设计预训练任务的原因，如果仅考虑节点层次，那么不同类型的节点(图中形状不一样的节点)的Embedding会相似，但是由这些节点组合而成的图Embedding不可分，相反如果只考虑图层次，虽然图Embedding可分了，但是学习到的节点Embedding却没有语义上的一致性(相同类型的节点在Embedding空间中不相近)，因此作者提出了要结合节点层次和图层次共同设计与训练任务。在这篇论文中，作者设计了3个预训练任务，和前一篇博文中我们介绍的预训练任务稍有不同。

## 预训练任务

### 任务1 上下文预测：学习可以捕获局部图结构的节点Embedding

大多数现有的无监督节点表示学习方法被设计用于节点分类，并且要求附近节点具有类似的 embeddings。这不适用于整个图的表示学习，其中捕获局部邻域的结构相似性更重要。

NLP 领域中提出了一种分布式假设(distributional hypothesis)，分布式假设的意思是如果词的含义相近，那么它们的上下文也应该相似，Word2vec就是成功利用这一假设的词向量表征方法。举个例子，"姚明” 和 "易建联” 都是NBA篮球运动员，所以他们可能会出现在介绍中国NBA球星的上下文中。

受这种分布式假设的启发，作者将分布式假设应用于复杂图领域中，以更好的训练 GNNs 预训练模型。这里需要解决以下两个问题：

- 如何在图领域中定义 "词” 和 "上下文”；
- 如何在预测问题中表示上下文。

下面将对上述问题的解决方法进行介绍。

#### 图领域 "词” 和 "上下文”的定义

如何将"词”的概念类比到图领域呢？由于GNN中每个节点的Embedding都是由其邻居节点更新的，因此作者采用节点 $v$ 的 $K$-hop 子图结构对$v$进行编码，最终获得的节点 embeddings $h_v^{(K)}$ 作为节点$v$的"词”表示。该子图结构（Substructure）如下图所示。如果左图中的红色节点是$v$，那么它的整个$K$-hop领域就是这个节点的"词”表示，根据GNN的聚合算法，$K$-hop领域内的所有邻居都将自身的Embedding聚合到$v$上来，形成$v$的"词向量"$h_v^{(k)}。

![Screen Shot 2019-08-11 at 2.24.29 PM](http://ww1.sinaimg.cn/large/006tNc79ly1g5vpyhsb1rj30ss0ca416.jpg)

类似地，类比到NLP上，上下文指词前后的内容，那么图中的上下文（Context）可以被定义为围绕 K-hop 子图的图结构。例如：左图中 $r_1$-hop 和 $r_2$-hop 间的节点形成了节点中间红色节点的上下文。这里定义 $r_1 < K$ ，这样一些节点在词和上下文之间可以共享。

#### 使用另一个 GNN 将上下文编码到一个固定向量中

定义完图上的”词"和”上下文"的概念之后，我们还需要将”词"和”上下文"表征出来，和NLP的任务不同，图中的”词"和"上下文”都是有结构的数据，因为他们都是子图的结构。在论文中，作者将这两个Embedding分别使用两个GNN进行编码，对于节点$v$，只需要使用传统GNN的聚合公式即可得到节点$v$的词向量表示(见右图上边部分)，再使用另一个GNN(记做GNN‘，见右图下边部分)对节点$v$的"上下文”结构中涉及到的节点($r_1$-hop和$r_2$-hop之间的节点)的Embedding做一个pooling来表征节点$v$的上下文信息。我们将节点$v$的"词"表征记做为$h_v^{(K)}$，而"上下文"表征记做为$c_v^{G^\prime}$，其中$G^\prime$表示用来对上下文进行编码的另一个GNN。

#### 通过负采样进行训练

得到节点的"词"表征和”上下文"表征之后，我们可以通过一个二分类任务进行学习

$$
\sigma\left(h_{v}^{(K) \top} c_{v^{\prime}}^{G^{\prime}}\right) \approx \mathbf{1}\left\{v \text { and } v^{\prime} \text { are the same node in } G\right\}
$$

其中$$\mathbf{1}(\cdot)$$ 是指示符函数，即：我们希望对于相同的节点，它的自身表征("词”表征)和它的邻域表征("上下文"表征)尽量相似，当然，对于不同节点的自身表征和邻域表征，我们希望它们尽量不相似，论文中通过负采样技术构建了这种负样本，在此不加赘述。

做一个小结，通过设计上下文预测的预训练任务，我们希望GNN能够将具有相似邻域的节点映射到相近的Embedding空间中，即使得学习到的节点Embedding能够捕获其邻域信息。

### 任务2 masking：训练节点 embeddings 以获得领域相关知识

任务2的设计和前面介绍的论文很相似，不同于前一篇博文中介绍的是，这篇论文的是对节点/边的某些属性进行masking，然后利用学习到的节点Embedding进行预测。下图展示了masking节点的任务，在论文所实验的分子结构的场景中，被masking的信息是节点的分子类型，然后通过节点Embedding来预测这些分子类型（碳、氮、氧，硫等）。同理，对于masking边的预训练任务，我们可以通过边的两个端点的Embedding来预测边的类型（单键、双键等）

![Screen Shot 2019-08-11 at 3.26.21 PM](http://ww1.sinaimg.cn/large/006tNc79ly1g5wp5sdva0j30d20cwt98.jpg)

做一个小结，设计masking预训练任务的目的是来让模型学习有意义节点Embedding，以反映出特定领域知识。如上图所示，通过对分子结构图进行掩码，GNNs 能够学习一些化学规则。

### 任务3 图预测任务：训练节点Embedding以组合成合理的全图Embedding

上文中我们介绍了仅设计节点预训练任务的弊端，这节作者将重点放在如何设计图级别预训练的方法来学习可组合的节点embeddings：

1) 通过对特定领域的图标签进行预测，比如这个图是什么或者属于那种类型，这其实是有监督的方式;
2) 通过对图级别结构进行自监督预测，例如图编辑距离或图结构相似性。

在论文中，作者主要关注第一种方式，其实就是一种有监督的训练方式来调整节点Embedding。

做一个小结，本节中介绍了利用预训练 GNNs 来生成可组合的节点 embeddings 。通过组合这些Embedding能够获得整个图的有效表示，以确保图 embeddings 并且可以适用到各种下游任务中。



